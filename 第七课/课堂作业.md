# 课堂作业
作业要求
练习nltk和jieba分词的使用。
制作中英文双语数据10对。
将上述数据进行分词。
创建自定义用户词典，模拟专业术语（术语库）。
使用加载自定义用户词典后的分词器对数据再进行分词。

# 安装
C:\Windows\system32>pip install jieba
Collecting jieba
  Downloading jieba-0.42.1.tar.gz (19.2 MB)
     |████████████████████████████████| 19.2 MB 29 kB/s
Using legacy 'setup.py install' for jieba, since package 'wheel' is not installed.
Installing collected packages: jieba
    Running setup.py install for jieba ... done
Successfully installed jieba-0.42.1

# 进入Python
C:\Windows\system32>python
Python 3.8.3 (tags/v3.8.3:6f8c832, May 13 2020, 22:20:19) [MSC v.1925 32 bit (Intel)] on win32
Type "help", "copyright", "credits" or "license" for more information.

# 引入结巴分词并测试
>>> seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
>>> print("Full Mode: " + "/ ".join(seg_list))  # 全模式
Building prefix dict from the default dictionary ...
Dumping model to file cache C:\Users\LI\AppData\Local\Temp\jieba.cache
Loading model cost 3.840 seconds.
Prefix dict has been built successfully.
Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学
>>> seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
>>> print("Default Mode: " + "/ ".join(seg_list))  # 精确模式
Default Mode: 我/ 来到/ 北京/ 清华大学
>>>
>>> seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
>>> print(", ".join(seg_list))
他, 来到, 了, 网易, 杭研, 大厦
>>>
>>> seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
>>> print(", ".join(seg_list))
小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造
>>>
>>> # 新词识别  "杭研"并没有在词典中,但是也被Viterbi算法识别出来了
>>> seg_list = jieba.cut("他来到了网易杭研大厦")
>>> print (u"[新词识别]: ", "/ ".join(seg_list))
[新词识别]:  他/ 来到/ 了/ 网易/ 杭研/ 大厦
>>>
